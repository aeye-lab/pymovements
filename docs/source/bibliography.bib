@article{BSC,
    author={Pan, Jinger and Yan, Ming and Richter, Eike M. and Shu, Hua and Kliegl, Reinhold},
    title={The {B}eijing {S}entence {C}orpus: A {C}hinese sentence corpus with eye movement data and predictability norms},
    journal={Behavior Research Methods},
    year={2022},
    volume={54},
    url={https://link.springer.com/article/10.3758/s13428-021-01730-2}, 
    doi={https://doi.org/10.3758/s13428-021-01730-2},
    issue={4},
}

@article{BSCII,
    author={Yan, Ming and Pan, Jinger and Kliegl, Reinhold},
    title={The {B}eijing {S}entence {C}orpus {II}: A cross-script comparison between traditional and simplified Chinese sentence reading},
    journal={Behavior Research Methods},
    year={2025},
    url={https://link.springer.com/article/10.3758/s13428-024-02523-z},
	doi={https://doi.org/10.3758/s13428-024-02523-z},
    volume={57},
    issue={2},
}

@article{CodeComprehension,
author = {Alakmeh, Tarek and Reich, David and J\"{a}ger, Lena and Fritz, Thomas},
title = {Predicting Code Comprehension: A Novel Approach to Align Human Gaze with Code using Deep Neural Networks},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660795},
doi = {10.1145/3660795},
abstract = {The better the code quality and the less complex the code, the easier it is for software developers to comprehend and evolve it. Yet, how do we best detect quality concerns in the code? Existing measures to assess code quality, such as McCabe’s cyclomatic complexity, are decades old and neglect the human aspect. Research has shown that considering how a developer reads and experiences the code can be an indicator of its quality. In our research, we built on these insights and designed, trained, and evaluated the first deep neural network that aligns a developer’s eye gaze with the code tokens the developer looks at to predict code comprehension and perceived difficulty. To train and analyze our approach, we performed an experiment in which 27 participants worked on a range of 16 short code comprehension tasks while we collected fine-grained gaze data using an eye tracker. The results of our evaluation show that our deep neural sequence model that integrates both the human gaze and the stimulus code, can predict (a) code comprehension and (b) the perceived code difficulty significantly better than current state-of-the-art reference methods. We also show that aligning human gaze with code leads to better performance than models that rely solely on either code or human gaze. We discuss potential applications and propose future work to build better human-inclusive code evaluation systems.},
journal = {Proc. ACM Softw. Eng.},
month = {jul},
articleno = {88},
numpages = {23},
keywords = {code comprehension, code-fixation attention, eye-tracking, lab experiment, neural networks}
}

@inproceedings{CopCoL1Hollenstein,
    title = "The {C}openhagen Corpus of Eye Tracking Recordings from Natural Reading of {D}anish Texts",
    author = {Hollenstein, Nora  and
      Barrett, Maria  and
      Bj{\"o}rnsd{\'o}ttir, Marina},
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.182",
    pages = "1712--1720",
}

@inproceedings{CopCoL1DysBjornsdottir,
    title = "Dyslexia Prediction from Natural Reading of {D}anish Texts",
    author = {Bj{\"o}rnsd{\'o}ttir, Marina  and
      Hollenstein, Nora  and
      Barrett, Maria},
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.7",
    pages = "60--70",
}

@inproceedings{CopCoL2,
    title = "Reading Does Not Equal Reading: Comparing, Simulating and Exploiting Reading Behavior across Populations",
    author = {Reich, David R.  and
      Deng, Shuwen  and
      Bj{\"o}rnsd{\'o}ttir, Marina  and
      J{\"a}ger, Lena  and
      Hollenstein, Nora},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1187",
    pages = "13586--13594",
}

@ARTICLE{DAEMONS,
    AUTHOR={Schwetlick, Lisa  and Kümmerer, Matthias  and Bethge, Matthias  and Engbert, Ralf},
    TITLE={Potsdam data set of eye movement on natural scenes (DAEMONS)},
    JOURNAL={Frontiers in Psychology},
    VOLUME={15},
    YEAR={2024},
    URL={https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1389609},
    DOI={10.3389/fpsyg.2024.1389609},
    ISSN={1664-1078},
}

@inproceedings{DIDEC,
    title = "{DIDEC}: The {D}utch Image Description and Eye-tracking Corpus",
    author = "van Miltenburg, Emiel  and
      K{\'a}d{\'a}r, {\'A}kos  and
      Koolen, Ruud  and
      Krahmer, Emiel",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1310",
    pages = "3658--3669",
}

@misc{EMTeC,
      title={EMTeC: A Corpus of Eye Movements on Machine-Generated Texts},
      author={Lena Sophia Bolliger and Patrick Haller and Isabelle Caroline Rose Cretton and David Robert Reich and Tannon Kew and Lena Ann Jäger},
      year={2024},
      eprint={2408.04289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.04289},
}

@article{EngbertKliegl2003,
    author = {Engbert, Ralf and Kliegl, Reinhold},
    title = {Microsaccades uncover the orientation of covert attention},
    journal = {Vision Research},
    volume = {43},
    number = {9},
    pages = {1035-1045},
    year = {2003},
    issn = {0042-6989},
    doi = {10.1016/S0042-6989(03)00084-1},
}

@misc{Engbert2015,
    author = {Engbert, Ralf and Sinn, Petra and Mergenthaler, Konstantin and Trukenbrod, Hans},
    title = {Microsaccade Toolbox 0.9},
    year = {2015},
    url = {http://read.psych.uni-potsdam.de/index.php?view=article&id=140},
}

@article{GazeBase,
    author = {Griffith, Henry and Lohr, Dillon and Abdulin, Evgeny and Komogortsev, Oleg},
    year = {2021},
    pages = {},
    title = {{GazeBase}, a large-scale, multi-stimulus, longitudinal eye movement dataset},
    volume = {8},
    journal = {Scientific Data},
    doi = {10.1038/s41597-021-00959-y},
}

@inproceedings{InteRead,
    title = "{I}nte{R}ead: An Eye Tracking Dataset of Interrupted Reading",
    author = {Zermiani, Francesca  and
      Dhar, Prajit  and
      Sood, Ekta  and
      K{\"o}gel, Fabian  and
      Bulling, Andreas  and
      Wirzberger, Maria},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.802",
    pages = "9154--9169",
    abstract = "Eye movements during reading offer a window into cognitive processes and language comprehension, but the scarcity of reading data with interruptions {--} which learners frequently encounter in their everyday learning environments {--} hampers advances in the development of intelligent learning technologies. We introduce InteRead {--} a novel 50-participant dataset of gaze data recorded during self-paced reading of real-world text. InteRead further offers fine-grained annotations of interruptions interspersed throughout the text as well as resumption lags incurred by these interruptions. Interruptions were triggered automatically once readers reached predefined target words. We validate our dataset by reporting interdisciplinary analyses on different measures of gaze behavior. In line with prior research, our analyses show that the interruptions as well as word length and word frequency effects significantly impact eye movements during reading. We also explore individual differences within our dataset, shedding light on the potential for tailored educational solutions. InteRead is accessible from our datasets web-page: https://www.ife.uni-stuttgart.de/en/llis/research/datasets/.",
 }

@misc{JuDo1000,
    author = {Makowski, Silvia and Jäger, Lena A. and Prasse, Paul and Scheffer, Tobias},
    title = {{JuDo1000} Eye Tracking Data Set},
    year = {2020},
    pages = {1-10},
    doi = {10.17605/OSF.IO/5ZPVK},
}

@misc{PoTeC,
    url = {https://github.com/DiLi-Lab/PoTeC},
    author = {Jakobi, Deborah N. and Kern, Thomas and Reich, David R. and Haller, Patrick and J\"ager, Lena A.},
    title = {{PoTeC}: A {G}erman Naturalistic Eye-tracking-while-reading Corpus},
    year = {2024},
    note = {under review},
}

@article{SalvucciGoldberg2000,
    author = {Salvucci, Dario D. and Goldberg, Joseph H.},
    title = {Identifying Fixations and Saccades in Eye-Tracking Protocols},
    journal = {Proceedings of the 2000 Symposium on Eye Tracking Research & Applications},
    pages = {71–78},
    year = {2000},
    issn = {1581132808},
    doi = {10.1145/355017.355028},
}

@article{SavitzkyGolay1964,
    author = {Savitzky, Abraham. and Golay, Marcel J. E.},
    title = {Smoothing and Differentiation of Data by Simplified Least Squares Procedures.},
    journal = {Analytical Chemistry},
    volume = {36},
    number = {8},
    pages = {1627-1639},
    year = {1964},
    doi = {10.1021/ac60214a047},
}

@article{GazeBaseVR,
    author = {Lohr, Dillon and Aziz, Samantha and Friedman, Lee and Komogortsev, Oleg},
    year = {2023},
    pages = {},
    title = {{GazeBaseVR}, a large-scale, longitudinal, binocular eye-tracking dataset collected in virtual reality},
    volume = {10},
    journal = {Scientific Data},
    doi = {10.1038/s41597-023-02075-5},
}

@article{GazeOnFaces,
    author = {Coutrot, Antoine and Binetti, Nicola and Harrison, Charlotte and Mareschal, Isabelle and Johnston, Alan},
    title = {Face exploration dynamics differentiate men and women},
    journal = {Journal of Vision},
    volume = {16},
    number = {14},
    pages = {16-16},
    year = {2016},
    month = {11},
    abstract = { The human face is central to our everyday social interactions. Recent studies have shown that while gazing at faces, each one of us has a particular eye-scanning pattern, highly stable across time. Although variables such as culture or personality have been shown to modulate gaze behavior, we still don't know what shapes these idiosyncrasies. Moreover, most previous observations rely on static analyses of small-sized eye-position data sets averaged across time. Here, we probe the temporal dynamics of gaze to explore what information can be extracted about the observers and what is being observed. Controlling for any stimuli effect, we demonstrate that among many individual characteristics, the gender of both the participant (gazer) and the person being observed (actor) are the factors that most influence gaze patterns during face exploration. We record and exploit the largest set of eye-tracking data (405 participants, 58 nationalities) from participants watching videos of another person. Using novel data-mining techniques, we show that female gazers follow a much more exploratory scanning strategy than males. Moreover, female gazers watching female actresses look more at the eye on the left side. These results have strong implications in every field using gaze-based models from computer vision to clinical psychology. },
    issn = {1534-7362},
    doi = {10.1167/16.14.16},
    url = {https://doi.org/10.1167/16.14.16},
}

@article{Provo,
author = {Luke, Steven G. and Christianson, Kiel},
address = {New York},
copyright = {Psychonomic Society, Inc. 2017},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Cognitive psychology ; Computational linguistics ; Female ; Human beings ; Information storage and retrieval systems ; Male ; Psychology ; Reading ; Semantics},
language = {eng},
number = {2},
abstract = {This article presents the Provo Corpus, a corpus of eye-tracking data with accompanying predictability norms. The predictability norms for the Provo Corpus differ from those of other corpora. In addition to traditional cloze scores that estimate the predictability of the full orthographic form of each word, the Provo Corpus also includes measures of the predictability of the morpho-syntactic and semantic information for each word. This makes the Provo Corpus ideal for studying predictive processes in reading. Some analyses using these data have previously been reported elsewhere (Luke \& Christianson,
2016
). The Provo Corpus is available for download on the Open Science Framework, at
https://osf.io/sjefs
.},
pages = {826-833},
publisher = {Springer US},
title = {The {P}rovo {C}orpus: A large eye-tracking corpus with predictability norms},
url={https://link.springer.com/article/10.3758/s13428-017-0908-4},
doi={https://doi.org/10.3758/s13428-017-0908-4},
volume = {50},
year = {2018},
}

@inproceedings{SB-SAT,
author = {Ahn, Seoyoung and Kelton, Conor and Balasubramanian, Aruna and Zelinsky, Greg},
title = {Towards Predicting Reading Comprehension From Gaze Behavior},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391335},
doi = {10.1145/3379156.3391335},
abstract = {As readers of a language, we all agree to move our eyes in roughly the same way. Yet might there be hidden within this self-similar behavior subtle clues as to how a reader is understanding the material being read? Here we attempt to decode a reader’s eye movements to predict their level of text comprehension and related states. Eye movements were recorded from 95 people reading 4 published SAT passages, each followed by corresponding SAT questions and self-evaluation questionnaires. A sequence of 21 fixation-location (x,y), fixation-duration, and pupil-size features were extracted from the reading behavior and input to two deep networks (CNN/RNN), which were used to predict the reader’s comprehension level and other comprehension-related variables. The best overall comprehension prediction accuracy was 65\% (cf. null accuracy = 54\%) obtained by CNN. This prediction generalized well to fixations on new passages (64\%) from the same readers, but did not generalize to fixations from new readers (41\%), implying substantial individual differences in reading behavior. Our work is the first attempt to predict comprehension from fixations using deep networks, where we hope that our large reading dataset and our protocol for evaluation will benefit the development of new methods for predicting reading comprehension by decoding gaze behavior.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {32},
numpages = {5},
keywords = {Eye tracking, Machine learning, Reading dataset, Text comprehension prediction},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}

@article{HBN,
  title={An open resource for transdiagnostic research in pediatric mental health and learning disorders},
  author={Alexander, Lindsay M. and
 Escalera, Jasmine and
 Ai, Lei and
 Andreotti, Charissa and
 Febre, Karina and
 Mangone, Alexander and
 Vega-Potler, Natan and
 Langer, Nicolas and
 Alexander, Alexis and
 Kovacs, Meagan and
 Litke, Shannon and
 O'Hagan, Bridget and
 Andersen, Jennifer and
 Bronstein, Batya and
 Bui, Anastasia and
 Bushey, Marijayne and
 Butler, Henry and
 Castagna, Victoria and
 Camacho, Nicolas and
 Chan, Elisha and
 Citera, Danielle and
 Clucas, Jon and
 Cohen, Samantha and
 Dufek, Sarah and
 Eaves, Megan and
 Fradera, Brian and
 Gardner, Judith and
 Grant-Villegas, Natalie and
 Green, Gabriella and
 Gregory, Camille and
 Hart, Emily and
 Harris, Shana and
 Horton, Megan and
 Kahn, Danielle and
 Kabotyanski, Katherine and
 Karmel, Bernard and
 Kelly, Simon P. and
 Kleinman, Kayla and
 Koo, Bonhwang and
 Kramer, Eliza and
 Lennon, Elizabeth and
 Lord, Catherine and
 Mantello, Ginny and
 Margolis, Amy and
 Merikangas, Kathleen R. and
 Milham, Judith and
 Minniti, Giuseppe and
 Neuhaus, Rebecca and
 Levine, Alexandra and
 Osman, Yael and
 Parra, Lucas C. and
 Pugh, Ken R. and
 Racanello, Amy and
 Restrepo, Anita and
 Saltzman, Tian and
 Septimus, Batya and
 Tobe, Russell and
 Waltz, Rachel and
 Williams, Anna and
 Yeo, Anna and
Castellanos, Francisco X. and
Klein, Arno and
Paus, Tomas and
Leventhal, Bennett L. and
Craddock, R. Cameron and
Koplewicz, Harold S. and
Milham, Michael P.},
  journal={Scientific data},
  volume={4},
  number={1},
  pages={1--26},
  url={https://www.nature.com/articles/sdata2017181},
  doi={https://doi.org/10.1038/sdata.2017.181},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{GazeGraph,
author = {Lan, Guohao and Heit, Bailey and Scargill, Tim and Gorlatova, Maria},
title = {{GazeGraph}: Graph-based few-shot cognitive context sensing from human visual behavior},
year = {2020},
isbn = {9781450375900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384419.3430774},
doi = {10.1145/3384419.3430774},
abstract = {In this work, we present GazeGraph, a system that leverages human gazes as the sensing modality for cognitive context sensing. GazeGraph is a generalized framework that is compatible with different eye trackers and supports various gaze-based sensing applications. It ensures high sensing performance in the presence of heterogeneity of human visual behavior, and enables quick system adaptation to unseen sensing scenarios with few-shot instances. To achieve these capabilities, we introduce the spatial-temporal gaze graphs and the deep learning-based representation learning method to extract powerful and generalized features from the eye movements for context sensing. Furthermore, we develop a few-shot gaze graph learning module that adapts the `learning to learn' concept from meta-learning to enable quick system adaptation in a data-efficient manner. Our evaluation demonstrates that GazeGraph outperforms the existing solutions in recognition accuracy by 45\% on average over three datasets. Moreover, in few-shot learning scenarios, GazeGraph outperforms the transfer learning-based approach by 19\% to 30\%, while reducing the system adaptation time by 80\%.},
booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems (SenSys)},
pages = {422–435},
numpages = {14},
keywords = {few-shot learning, eye tracking, cognitive context sensing},
location = {Virtual Event, Japan},
series = {SenSys '20}
}

@article{FakeNewsPerception,
author = {Sümer, Ömer and Bozkir, Efe and Kübler, Thomas and Grüner, Sven and Utz, Sonja and Kasneci, Enkelejda},
year = {2021},
month = {03},
pages = {106909},
title = {FakeNewsPerception: An Eye Movement Dataset on the Perceived Believability of News Stories},
volume = {35},
journal = {Data in Brief},
doi = {10.1016/j.dib.2021.106909}
}

@article{UCL,
    author = {
        Frank, Stefan L. and
        Fernandez Monsalve, Irene and
        Thompson, Robin L. and
        Vigliocco, Gabriella
    },
    year = {2013},
    title = {Reading time data for evaluating broad-coverage models of {E}nglish sentence processing},
    journal = {Behavior Research Methods},
    volume = {45},
    issue = {4},
    doi = {10.3758/s13428-012-0313-y},
}

@inproceedings{Gaze4Hate,
    title = "Eyes Don`t Lie: Subjective Hate Annotation and Detection with Gaze",
    author = {Alacam, {\"O}zge  and
      Hoeken, Sanne  and
      Zarrie{\ss}, Sina},
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.11/",
    doi = "10.18653/v1/2024.emnlp-main.11",
    pages = "187--205",
    abstract = "Hate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators' subjective hate ratings and improve predictions of text-only hate speech models."
}

@inproceedings{PotsdamBingePVT,
    author = {
        Prasse, Paul and
        Reich, David R. and
        Chwastek, Jakob and
        Makowski, Silvia and
        Jäger, Lena A. and
        Scheffer, Tobias
    },
	booktitle = {Proceedings of the ACM Symposium on Eye Tracking Research and Applications},
    year = {2025},
    title = {Detection of Alcohol Inebriation from Eye Movements using Remote and Wearable Eye Trackers},
    doi = {10.1145/3715669.3723109},
}

@article{MouseCursor,
    title = {Gaze tracking dataset for comparison of smooth and saccadic eye tracking},
    journal = {Data in Brief},
    volume = {34},
    pages = {106730},
    year = {2021},
    issn = {2352-3409},
    doi = {https://doi.org/10.1016/j.dib.2021.106730},
    url = {https://www.sciencedirect.com/science/article/pii/S2352340921000160},
    author = {Adam Pantanowitz and Kimoon Kim and Chelsey Chewins and David M Rubin},
    keywords = {Eye movement, Saccadic eye tracking, Smooth eye tracking, Image processing, Pupil detection, Vestibulo-ocular reflex},
    abstract = {Pupil tracking data are collected through the use of an infrared camera, and a head-mounted system [1]. The head-mounted system detects the relative pupil position and adjusts the mouse cursor position accordingly. The data are available for comparison of eye tracking with saccadic movements (with the head fixed in space) versus those from smooth movements (with the head moving in space). The analysis comprises two experiments for both types of eye tracking, which are performed with ten trials each for two participants. In the first experiment, the participant attempts to place the cursor into a target boundary of varying sizes. In the second experiment, the participant attempts to move the cursor to a target location within the shortest time.}
}

@article{MECOL1W1,
  title={{Expanding horizons of cross-linguistic research on reading: The Multilingual Eye-movement Corpus (MECO)}},
  author={Siegelman, Noam and Schroeder, Sascha and Acart{\"u}rk, Cengiz and Ahn, Hee-Don and Alexeeva, Svetlana and Amenta, Simona and Bertram, Raymond and Bonandrini, Rolando and Brysbaert, Marc and Chernova, Daria and others},
  journal={Behavior research methods},
  volume={54},
  number={6},
  pages={2843--2863},
  year={2022},
  publisher={Springer},
doi={10.3758/s13428-021-01772-6}
}

@article{MECOL2W1,
    title={Text reading in English as a second language: Evidence from the Multilingual Eye-Movements Corpus},
    volume={45},
    DOI={10.1017/S0272263121000954},
    number={1},
    journal={Studies in Second Language Acquisition},
    author={Kuperman, Victor and Siegelman, Noam and Schroeder, Sascha and Acartürk, Cengiz and Alexeeva, Svetlana and Amenta, Simona and Bertram, Raymond and Bonandrini, Rolando and Brysbaert, Marc and Chernova, Daria and et al.},
    year={2023},
    pages={3–37}
}

@article{MECOL2W2,
    title={New data on text reading in English as a second language: The Wave 2 expansion of the Multilingual Eye-Movement Corpus (MECO)},
    DOI={10.1017/S0272263125000105},
    journal={Studies in Second Language Acquisition},
    author={Kuperman, Victor and Schroeder, Sascha and Acartürk, Cengiz and Agrawal, Niket and Alexandre, Dominick M. and Bolliger, Lena S. and Brasser, Jan and Campos-Rojas, César and Drieghe, Denis and Đurđević, Dušica Filipović and et al.}, year={2025},
    pages={1–19}
}

@inproceedings{IITB_HGC,
    title = "Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection",
    author = "Maharaj, Kishan  and
      Saxena, Ashita  and
      Kumar, Raja  and
      Mishra, Abhijit  and
      Bhattacharyya, Pushpak",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.764/",
    doi = "10.18653/v1/2023.findings-emnlp.764",
    pages = "11424--11438",
    abstract = "Detecting hallucinations in natural language processing (NLP) is a critical undertaking that demands a deep understanding of both the semantic and pragmatic aspects of languages. Cognitive approaches that leverage users' behavioural signals, such as gaze, have demonstrated effectiveness in addressing NLP tasks with similar linguistic complexities. However, their potential in the context of hallucination detection remains largely unexplored. In this paper, we propose a novel cognitive approach for hallucination detection that leverages gaze signals from humans. We first collect and introduce an eye tracking corpus (IITB-HGC: IITB-Hallucination Gaze corpus) consisting of 500 instances, annotated by five annotators for hallucination detection. Our analysis reveals that humans selectively attend to relevant parts of the text based on distributional similarity, similar to the attention bias phenomenon in psychology. We identify two attention strategies employed by humans: global attention, which focuses on the most informative sentence, and local attention, which focuses on important words within a sentence. Leveraging these insights, we propose a novel cognitive framework for hallucination detection that incorporates these attention biases. Experimental evaluations on the FactCC dataset demonstrate the efficacy of our approach, obtaining a balanced accuracy of 87.1{\%}. Our study highlights the potential of gaze-based approaches in addressing the task of hallucination detection and sheds light on the cognitive processes employed by humans in identifying inconsistencies."
}

@article{ChineseReading,
  title = {The database of eye-movement measures on words in {Chinese} reading},
  volume = {9},
  number={1},
  ISSN = {2052–4463},
  DOI = {10.1038/s41597-022-01464-6},
  journal = {Scientific Data},
  publisher = {Springer Science and Business Media LLC},
  author = {Zhang,  Guangyao and Yao,  Panpan and Ma,  Guojie and Wang,  Jingwen and Zhou,  Junyi and Huang,  Linjieqiong and Xu,  Pingping and Chen,  Lijing and Chen,  Songlin and Gu,  Junjuan and Wei,  Wei and Cheng,  Xi and Hua,  Huimin and Liu,  Pingping and Lou,  Ya and Shen,  Wei and Bao,  Yaqian and Liu,  Jiayu and Lin,  Nan and Li,  Xingshan},
  year = {2022},
  pages={1-8}
}

@inproceedings{CoLAGaze,
    author = {Bondar, Anna and Reich, David R. and J\"{a}ger, Lena A.},
    title = {CoLAGaze: A Corpus of Eye Movements for Linguistic Acceptability},
    year = {2025},
    isbn = {979-8-4007-1487-0/2025/05},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3715669.3723120},
    booktitle = {2025 Symposium on Eye Tracking Research and Applications},
    numpages = {9},
    keywords = {Eye-tracking, reading, dataset, grammatical acceptability judgments},
    location = {Tokyo, Japan},
    series = {ETRA '25}
}

@article{OneStop,
    title={OneStop: A 360-Participant English Eye Tracking Dataset with Different Reading Regimes},
    author={Berzak, Yevgeni and Malmaud, Jonathan and Shubi, Omer and Meiri, Yoav and Lion, Ella and Levy, Roger},
    journal={PsyArXiv preprint},
	url={https://osf.io/preprints/psyarxiv/kgxv5_v2},
    doi={https://doi.org/10.31234/osf.io/kgxv5_v2},
    year={2025}
}

@InProceedings{ETDD70,
    author={Sedmidubsky, Jan and Dostalova, Nicol and Svaricek, Roman and Culemann, Wolf},
    editor={Ch{\'a}vez, Edgar and Kimia, Benjamin and Loko{\v{c}}, Jakub and Patella, Marco and Sedmidubsky, Jan},
    title="ETDD70: Eye-Tracking Dataset for Classification of Dyslexia Using AI-Based Methods",
    booktitle="Similarity Search and Applications",
    year="2025",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="34--48",
    url={https://link.springer.com/chapter/10.1007/978-3-031-75823-2_3},
    doi={https://doi.org/10.1007/978-3-031-75823-2_3},
    abstract="Dyslexia, a specific learning disorder, poses challenges in reading and language processing. Traditional diagnostic methods often rely on subjective assessments, leading to inaccuracies and delays in intervention. This work proposes classifying dyslexia using AI-based methods applied to eye-tracking data captured during text reading tasks. To facilitate future research in this domain, we collect a novel dataset (ETDD70) comprising eye-tracking recordings of 70 individuals for three reading tasks. In particular, the dataset contains high-frequency and accurate time series of 2D positions of eye movements and many derived characteristics extracted from eye movement patterns. By leveraging similarity-search approaches and deep learning models, we demonstrate the utility of such data in training several classification models, the best of which can distinguish between dyslexic and non-dyslexic individuals with an accuracy of around 90 {\%}. Both the dataset and evaluated models provide a valuable resource for researchers to further advance AI-based methods for dyslexia classification.",
    isbn="978-3-031-75823-2"
}

@article{TECO,
    title = {TECO: An Eye-tracking Corpus of Japanese L2 English Learners’ Text Reading},
    journal = {Research Methods in Applied Linguistics},
    volume = {3},
    number = {2},
    pages = {100123},
    year = {2024},
    issn = {2772-7661},
    doi = {https://doi.org/10.1016/j.rmal.2024.100123},
    url = {https://www.sciencedirect.com/science/article/pii/S2772766124000296},
    author = {Shingo Nahatame and Tomoko Ogiso and Yukino Kimura and Yuji Ushiro},
    keywords = {Second language, Reading, Eye tracking, Corpus},
    abstract = {Eye-tracking corpora are invaluable resources for understanding human text processing. In recent years, some corpora have been developed that incorporate second-language (L2) English readers’ eye-movement recordings. However, these face limitations such as small data sizes, the absence of multiple and longer text sources, and a scarcity of data from learners whose first languages are linguistically distinct from English. Addressing these gaps, this study introduces Tsukuba Eye-tracking Corpus (TECO), a dataset of eye-tracking records from Japanese L2 English learners engaged in text reading. TECO encompasses eye-tracking data for over 410,000 tokens, collected from 41 Japanese students who each read 30 English passages ranging in length from 300–400 words. In this article, we detail the design of TECO and report on the reliability of commonly used eye-tracking measures (e.g., skipping, first fixation duration, and regression) along with their descriptive statistics and distribution. We also validate the corpus by illustrating the impact of several lexical and reader factors (e.g., word length and reading proficiency) on some eye-tracking measures. TECO will serve as a valuable resource for researchers who are keen on exploring the cognitive processes involved in L2 reading. The corpus is freely accessible at the Open Science Framework [https://osf.io/wrvj3/], and we are committed to its continuous expansion by adding participants from diverse backgrounds and incorporating more detailed text information.}
}
