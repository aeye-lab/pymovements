@article{BSC,
    author={Pan, Jinger and Yan, Ming and Richter, Eike M. and Shu, Hua and Kliegl, Reinhold},
    title={The {B}eijing {S}entence {C}orpus: A {C}hinese sentence corpus with eye movement data and predictability norms},
    journal={Behavior Research Methods},
    year={2022},
    volume={54},
    issue={4},
}

@article{BSCII,
    author={Yan, Ming and Pan, Jinger and Kliegl, Reinhold},
    title={The {B}eijing {S}entence {C}orpus {II}: A cross-script comparison between traditional and simplified Chinese sentence reading},
    journal={Behavior Research Methods},
    year={2025},
    volume={57},
    issue={2},
}

@article{CodeComprehension,
author = {Alakmeh, Tarek and Reich, David and J\"{a}ger, Lena and Fritz, Thomas},
title = {Predicting Code Comprehension: A Novel Approach to Align Human Gaze with Code using Deep Neural Networks},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660795},
doi = {10.1145/3660795},
abstract = {The better the code quality and the less complex the code, the easier it is for software developers to comprehend and evolve it. Yet, how do we best detect quality concerns in the code? Existing measures to assess code quality, such as McCabe’s cyclomatic complexity, are decades old and neglect the human aspect. Research has shown that considering how a developer reads and experiences the code can be an indicator of its quality. In our research, we built on these insights and designed, trained, and evaluated the first deep neural network that aligns a developer’s eye gaze with the code tokens the developer looks at to predict code comprehension and perceived difficulty. To train and analyze our approach, we performed an experiment in which 27 participants worked on a range of 16 short code comprehension tasks while we collected fine-grained gaze data using an eye tracker. The results of our evaluation show that our deep neural sequence model that integrates both the human gaze and the stimulus code, can predict (a) code comprehension and (b) the perceived code difficulty significantly better than current state-of-the-art reference methods. We also show that aligning human gaze with code leads to better performance than models that rely solely on either code or human gaze. We discuss potential applications and propose future work to build better human-inclusive code evaluation systems.},
journal = {Proc. ACM Softw. Eng.},
month = {jul},
articleno = {88},
numpages = {23},
keywords = {code comprehension, code-fixation attention, eye-tracking, lab experiment, neural networks}
}

@inproceedings{CopCoL1Hollenstein,
    title = "The {C}openhagen Corpus of Eye Tracking Recordings from Natural Reading of {D}anish Texts",
    author = {Hollenstein, Nora  and
      Barrett, Maria  and
      Bj{\"o}rnsd{\'o}ttir, Marina},
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.182",
    pages = "1712--1720",
}

@inproceedings{CopCoL1DysBjornsdottir,
    title = "Dyslexia Prediction from Natural Reading of {D}anish Texts",
    author = {Bj{\"o}rnsd{\'o}ttir, Marina  and
      Hollenstein, Nora  and
      Barrett, Maria},
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.7",
    pages = "60--70",
}

@inproceedings{CopCoL2,
    title = "Reading Does Not Equal Reading: Comparing, Simulating and Exploiting Reading Behavior across Populations",
    author = {Reich, David R.  and
      Deng, Shuwen  and
      Bj{\"o}rnsd{\'o}ttir, Marina  and
      J{\"a}ger, Lena  and
      Hollenstein, Nora},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1187",
    pages = "13586--13594",
}

@ARTICLE{DAEMONS,
    AUTHOR={Schwetlick, Lisa  and Kümmerer, Matthias  and Bethge, Matthias  and Engbert, Ralf},
    TITLE={Potsdam data set of eye movement on natural scenes (DAEMONS)},
    JOURNAL={Frontiers in Psychology},
    VOLUME={15},
    YEAR={2024},
    URL={https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1389609},
    DOI={10.3389/fpsyg.2024.1389609},
    ISSN={1664-1078},
}

@inproceedings{DIDEC,
    title = "{DIDEC}: The {D}utch Image Description and Eye-tracking Corpus",
    author = "van Miltenburg, Emiel  and
      K{\'a}d{\'a}r, {\'A}kos  and
      Koolen, Ruud  and
      Krahmer, Emiel",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1310",
    pages = "3658--3669",
}

@misc{EMTeC,
      title={EMTeC: A Corpus of Eye Movements on Machine-Generated Texts},
      author={Lena Sophia Bolliger and Patrick Haller and Isabelle Caroline Rose Cretton and David Robert Reich and Tannon Kew and Lena Ann Jäger},
      year={2024},
      eprint={2408.04289},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.04289},
}

@article{EngbertKliegl2003,
    author = {Engbert, Ralf and Kliegl, Reinhold},
    title = {Microsaccades uncover the orientation of covert attention},
    journal = {Vision Research},
    volume = {43},
    number = {9},
    pages = {1035-1045},
    year = {2003},
    issn = {0042-6989},
    doi = {10.1016/S0042-6989(03)00084-1},
}

@misc{Engbert2015,
    author = {Engbert, Ralf and Sinn, Petra and Mergenthaler, Konstantin and Trukenbrod, Hans},
    title = {Microsaccade Toolbox 0.9},
    year = {2015},
    url = {http://read.psych.uni-potsdam.de/index.php?view=article&id=140},
}

@article{GazeBase,
    author = {Griffith, Henry and Lohr, Dillon and Abdulin, Evgeny and Komogortsev, Oleg},
    year = {2021},
    pages = {},
    title = {{GazeBase}, a large-scale, multi-stimulus, longitudinal eye movement dataset},
    volume = {8},
    journal = {Scientific Data},
    doi = {10.1038/s41597-021-00959-y},
}

@inproceedings{InteRead,
    title = "{I}nte{R}ead: An Eye Tracking Dataset of Interrupted Reading",
    author = {Zermiani, Francesca  and
      Dhar, Prajit  and
      Sood, Ekta  and
      K{\"o}gel, Fabian  and
      Bulling, Andreas  and
      Wirzberger, Maria},
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.802",
    pages = "9154--9169",
    abstract = "Eye movements during reading offer a window into cognitive processes and language comprehension, but the scarcity of reading data with interruptions {--} which learners frequently encounter in their everyday learning environments {--} hampers advances in the development of intelligent learning technologies. We introduce InteRead {--} a novel 50-participant dataset of gaze data recorded during self-paced reading of real-world text. InteRead further offers fine-grained annotations of interruptions interspersed throughout the text as well as resumption lags incurred by these interruptions. Interruptions were triggered automatically once readers reached predefined target words. We validate our dataset by reporting interdisciplinary analyses on different measures of gaze behavior. In line with prior research, our analyses show that the interruptions as well as word length and word frequency effects significantly impact eye movements during reading. We also explore individual differences within our dataset, shedding light on the potential for tailored educational solutions. InteRead is accessible from our datasets web-page: https://www.ife.uni-stuttgart.de/en/llis/research/datasets/.",
 }

@misc{JuDo1000,
    author = {Makowski, Silvia and Jäger, Lena A. and Prasse, Paul and Scheffer, Tobias},
    title = {{JuDo1000} Eye Tracking Data Set},
    year = {2020},
    pages = {1-10},
    doi = {10.17605/OSF.IO/5ZPVK},
}

@misc{PoTeC,
    url = {https://github.com/DiLi-Lab/PoTeC},
    author = {Jakobi, Deborah N. and Kern, Thomas and Reich, David R. and Haller, Patrick and J\"ager, Lena A.},
    title = {{PoTeC}: A {G}erman Naturalistic Eye-tracking-while-reading Corpus},
    year = {2024},
    note = {under review},
}

@article{SalvucciGoldberg2000,
    author = {Salvucci, Dario D. and Goldberg, Joseph H.},
    title = {Identifying Fixations and Saccades in Eye-Tracking Protocols},
    journal = {Proceedings of the 2000 Symposium on Eye Tracking Research & Applications},
    pages = {71–78},
    year = {2000},
    issn = {1581132808},
    doi = {10.1145/355017.355028},
}

@article{SavitzkyGolay1964,
    author = {Savitzky, Abraham. and Golay, Marcel J. E.},
    title = {Smoothing and Differentiation of Data by Simplified Least Squares Procedures.},
    journal = {Analytical Chemistry},
    volume = {36},
    number = {8},
    pages = {1627-1639},
    year = {1964},
    doi = {10.1021/ac60214a047},
}

@article{GazeBaseVR,
    author = {Lohr, Dillon and Aziz, Samantha and Friedman, Lee and Komogortsev, Oleg},
    year = {2023},
    pages = {},
    title = {{GazeBaseVR}, a large-scale, longitudinal, binocular eye-tracking dataset collected in virtual reality},
    volume = {10},
    journal = {Scientific Data},
    doi = {10.1038/s41597-023-02075-5},
}

@article{GazeOnFaces,
  title={Face exploration dynamics differentiate men and women},
  author={Coutrot, Antoine and Binetti, Nicola and Harrison, Charlotte and Mareschal, Isabelle and Johnston, Alan},
  journal={Journal of vision},
  volume={16},
  number={14},
  pages={16--16},
  year={2016},
  publisher={The Association for Research in Vision and Ophthalmology},
}

@article{Provo,
author = {Luke, Steven G. and Christianson, Kiel},
address = {New York},
copyright = {Psychonomic Society, Inc. 2017},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Cognitive psychology ; Computational linguistics ; Female ; Human beings ; Information storage and retrieval systems ; Male ; Psychology ; Reading ; Semantics},
language = {eng},
number = {2},
abstract = {This article presents the Provo Corpus, a corpus of eye-tracking data with accompanying predictability norms. The predictability norms for the Provo Corpus differ from those of other corpora. In addition to traditional cloze scores that estimate the predictability of the full orthographic form of each word, the Provo Corpus also includes measures of the predictability of the morpho-syntactic and semantic information for each word. This makes the Provo Corpus ideal for studying predictive processes in reading. Some analyses using these data have previously been reported elsewhere (Luke \& Christianson,
2016
). The Provo Corpus is available for download on the Open Science Framework, at
https://osf.io/sjefs
.},
pages = {826-833},
publisher = {Springer US},
title = {The {P}rovo {C}orpus: A large eye-tracking corpus with predictability norms},
volume = {50},
year = {2018},
}


@inproceedings{SB-SAT,
    title = {Towards predicting reading comprehension from gaze behavior},
    year = {2020},
    booktitle = {Proceedings of the ACM Symposium on Eye Tracking Research and Applications},
    author = {Ahn, Seoyoung and Kelton, Conor and Balasubramanian, Aruna and Zelinsky, Greg},
    pages = {1--5},
    publisher = {Association for Computing Machinery},
    address = "Stuttgart, Germany",
}

@article{HBN,
  title={An open resource for transdiagnostic research in pediatric mental health and learning disorders},
  author={Alexander, Lindsay M. and
 Escalera, Jasmine and
 Ai, Lei and
 Andreotti, Charissa and
 Febre, Karina and
 Mangone, Alexander and
 Vega-Potler, Natan and
 Langer, Nicolas and
 Alexander, Alexis and
 Kovacs, Meagan and
 Litke, Shannon and
 O'Hagan, Bridget and
 Andersen, Jennifer and
 Bronstein, Batya and
 Bui, Anastasia and
 Bushey, Marijayne and
 Butler, Henry and
 Castagna, Victoria and
 Camacho, Nicolas and
 Chan, Elisha and
 Citera, Danielle and
 Clucas, Jon and
 Cohen, Samantha and
 Dufek, Sarah and
 Eaves, Megan and
 Fradera, Brian and
 Gardner, Judith and
 Grant-Villegas, Natalie and
 Green, Gabriella and
 Gregory, Camille and
 Hart, Emily and
 Harris, Shana and
 Horton, Megan and
 Kahn, Danielle and
 Kabotyanski, Katherine and
 Karmel, Bernard and
 Kelly, Simon P. and
 Kleinman, Kayla and
 Koo, Bonhwang and
 Kramer, Eliza and
 Lennon, Elizabeth and
 Lord, Catherine and
 Mantello, Ginny and
 Margolis, Amy and
 Merikangas, Kathleen R. and
 Milham, Judith and
 Minniti, Giuseppe and
 Neuhaus, Rebecca and
 Levine, Alexandra and
 Osman, Yael and
 Parra, Lucas C. and
 Pugh, Ken R. and
 Racanello, Amy and
 Restrepo, Anita and
 Saltzman, Tian and
 Septimus, Batya and
 Tobe, Russell and
 Waltz, Rachel and
 Williams, Anna and
 Yeo, Anna and
Castellanos, Francisco X. and
Klein, Arno and
Paus, Tomas and
Leventhal, Bennett L. and
Craddock, R. Cameron and
Koplewicz, Harold S. and
Milham, Michael P.},
  journal={Scientific data},
  volume={4},
  number={1},
  pages={1--26},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{GazeGraph,
  title={{GazeGraph}: Graph-based few-shot cognitive context sensing from human visual behavior},
  author={Lan, Guohao and Heit, Bailey and Scargill, Tim and Gorlatova, Maria},
  booktitle={Proceedings of the 18th ACM Conference on Embedded Networked Sensor Systems (SenSys)},
  year={2020}
}

@article{FakeNewsPerception,
author = {Sümer, Ömer and Bozkir, Efe and Kübler, Thomas and Grüner, Sven and Utz, Sonja and Kasneci, Enkelejda},
year = {2021},
month = {03},
pages = {106909},
title = {FakeNewsPerception: An Eye Movement Dataset on the Perceived Believability of News Stories},
volume = {35},
journal = {Data in Brief},
doi = {10.1016/j.dib.2021.106909}
}

@article{UCL,
    author = {
        Frank, Stefan L. and
        Fernandez Monsalve, Irene and
        Thompson, Robin L. and
        Vigliocco, Gabriella
    },
    year = {2013},
    title = {Reading time data for evaluating broad-coverage models of {E}nglish sentence processing},
    journal = {Behavior Research Methods},
    volume = {45},
    issue = {4},
    doi = {10.3758/s13428-012-0313-y},
}

@inproceedings{PotsdamBingePVT,
    author = {
        Prasse, Paul and
        Reich, David R. and
        Chwastek, Jakob and
        Makowski, Silvia and
        Jäger, Lena A. and
        Scheffer, Tobias
    },
	booktitle = {Proceedings of the ACM Symposium on Eye Tracking Research and Applications},
    year = {2025},
    title = {Detection of Alcohol Inebriation from Eye Movements using Remote and Wearable Eye Trackers},
    doi = {10.1145/3715669.3723109},
}

@article{MouseCursor,
    title = {Gaze tracking dataset for comparison of smooth and saccadic eye tracking},
    journal = {Data in Brief},
    volume = {34},
    pages = {106730},
    year = {2021},
    issn = {2352-3409},
    doi = {https://doi.org/10.1016/j.dib.2021.106730},
    url = {https://www.sciencedirect.com/science/article/pii/S2352340921000160},
    author = {Adam Pantanowitz and Kimoon Kim and Chelsey Chewins and David M Rubin},
    keywords = {Eye movement, Saccadic eye tracking, Smooth eye tracking, Image processing, Pupil detection, Vestibulo-ocular reflex},
    abstract = {Pupil tracking data are collected through the use of an infrared camera, and a head-mounted system [1]. The head-mounted system detects the relative pupil position and adjusts the mouse cursor position accordingly. The data are available for comparison of eye tracking with saccadic movements (with the head fixed in space) versus those from smooth movements (with the head moving in space). The analysis comprises two experiments for both types of eye tracking, which are performed with ten trials each for two participants. In the first experiment, the participant attempts to place the cursor into a target boundary of varying sizes. In the second experiment, the participant attempts to move the cursor to a target location within the shortest time.}
}

@inproceedings{IITB_HGC,
    title = "Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection",
    author = "Maharaj, Kishan  and
      Saxena, Ashita  and
      Kumar, Raja  and
      Mishra, Abhijit  and
      Bhattacharyya, Pushpak",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.764/",
    doi = "10.18653/v1/2023.findings-emnlp.764",
    pages = "11424--11438",
    abstract = "Detecting hallucinations in natural language processing (NLP) is a critical undertaking that demands a deep understanding of both the semantic and pragmatic aspects of languages. Cognitive approaches that leverage users' behavioural signals, such as gaze, have demonstrated effectiveness in addressing NLP tasks with similar linguistic complexities. However, their potential in the context of hallucination detection remains largely unexplored. In this paper, we propose a novel cognitive approach for hallucination detection that leverages gaze signals from humans. We first collect and introduce an eye tracking corpus (IITB-HGC: IITB-Hallucination Gaze corpus) consisting of 500 instances, annotated by five annotators for hallucination detection. Our analysis reveals that humans selectively attend to relevant parts of the text based on distributional similarity, similar to the attention bias phenomenon in psychology. We identify two attention strategies employed by humans: global attention, which focuses on the most informative sentence, and local attention, which focuses on important words within a sentence. Leveraging these insights, we propose a novel cognitive framework for hallucination detection that incorporates these attention biases. Experimental evaluations on the FactCC dataset demonstrate the efficacy of our approach, obtaining a balanced accuracy of 87.1{\%}. Our study highlights the potential of gaze-based approaches in addressing the task of hallucination detection and sheds light on the cognitive processes employed by humans in identifying inconsistencies."
}

@article{ChineseReading,
  title = {The database of eye-movement measures on words in {Chinese} reading},
  volume = {9},
  number={1},
  ISSN = {2052–4463},
  DOI = {10.1038/s41597-022-01464-6},
  journal = {Scientific Data},
  publisher = {Springer Science and Business Media LLC},
  author = {Zhang,  Guangyao and Yao,  Panpan and Ma,  Guojie and Wang,  Jingwen and Zhou,  Junyi and Huang,  Linjieqiong and Xu,  Pingping and Chen,  Lijing and Chen,  Songlin and Gu,  Junjuan and Wei,  Wei and Cheng,  Xi and Hua,  Huimin and Liu,  Pingping and Lou,  Ya and Shen,  Wei and Bao,  Yaqian and Liu,  Jiayu and Lin,  Nan and Li,  Xingshan},
  year = {2022},
  pages={1-8}
}

@inproceedings{CoLAGaze,
    author = {Bondar, Anna and Reich, David R. and J\"{a}ger, Lena A.},
    title = {CoLAGaze: A Corpus of Eye Movements for Linguistic Acceptability},
    year = {2025},
    isbn = {979-8-4007-1487-0/2025/05},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/3715669.3723120},
    booktitle = {2025 Symposium on Eye Tracking Research and Applications},
    numpages = {9},
    keywords = {Eye-tracking, reading, dataset, grammatical acceptability judgments},
    location = {Tokyo, Japan},
    series = {ETRA '25}
}

@article{OneStop,
    title={OneStop: A 360-Participant English Eye Tracking Dataset with Different Reading Regimes},
    author={Berzak, Yevgeni and Malmaud, Jonathan and Shubi, Omer and Meiri, Yoav and Lion, Ella and Levy, Roger},
    journal={PsyArXiv preprint},
    year={2025}
}
